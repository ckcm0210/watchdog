# -*- coding: utf-8 -*-
"""
Created on Thu Jul 10 16:29:38 2025

@author: kccheng
"""

import os
import time
import csv
import hashlib
import gc
import psutil
import shutil
import gzip
import json
import signal
import threading
from datetime import datetime
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from openpyxl import load_workbook

delayed_checks = {}

# =========== User Config ============
SCAN_ALL_MODE = True

# ğŸš€ å„ªåŒ–é¸é …
USE_LOCAL_CACHE = True
ENABLE_FAST_MODE = True
CACHE_FOLDER = r"C:\Users\user\Desktop\pytest\cache"

# ğŸ”§ è¨ºæ–·å’Œæ¢å¾©é¸é …
ENABLE_TIMEOUT = True          # å•Ÿç”¨è¶…æ™‚ä¿è­·
FILE_TIMEOUT_SECONDS = 120     # æ¯å€‹æª”æ¡ˆæœ€å¤§è™•ç†æ™‚é–“ (ç§’)
ENABLE_MEMORY_MONITOR = True   # å•Ÿç”¨è¨˜æ†¶é«”ç›£æ§
MEMORY_LIMIT_MB = 2048         # è¨˜æ†¶é«”é™åˆ¶ (MB)
ENABLE_RESUME = True           # å•Ÿç”¨æ–·é»çºŒå‚³
RESUME_LOG_FILE = r"C:\Users\user\Desktop\pytest\baseline_progress.log"  # é€²åº¦è¨˜éŒ„æª”

WATCH_FOLDERS = [
    r"C:\Users\user\Desktop\pytest\æ–°å¢è³‡æ–™å¤¾ (5)"
]

MANUAL_BASELINE_TARGET = []

LOG_FOLDER = r"C:\Users\user\Desktop\pytest\excel_watch_log"
LOG_FILE_DATE = datetime.now().strftime('%Y%m%d')
CSV_LOG_FILE = os.path.join(LOG_FOLDER, f"excel_change_log_{LOG_FILE_DATE}.csv.gz")
SUPPORTED_EXTS = ('.xlsx', '.xlsm')

MAX_RETRY = 10
RETRY_INTERVAL_SEC = 2
USE_TEMP_COPY = True

WHITELIST_USERS = ['ckcm0210', 'yourwhiteuser']
LOG_WHITELIST_USER_CHANGE = True

FORCE_BASELINE_ON_FIRST_SEEN = [
    r"\\network_drive\\your_folder1\\must_first_baseline.xlsx",
    "force_this_file.xlsx"
]
# =========== End User Config ============

# å…¨å±€è®Šæ•¸
current_processing_file = None
processing_start_time = None
force_stop = False
baseline_completed = False

def signal_handler(signum, frame):
    """è™•ç† Ctrl+C ä¸­æ–·"""
    global force_stop
    if not force_stop:
        force_stop = True
        print("\nğŸ›‘ æ”¶åˆ°ä¸­æ–·ä¿¡è™Ÿï¼Œæ­£åœ¨å®‰å…¨åœæ­¢...")
        if current_processing_file:
            print(f"   ç›®å‰è™•ç†æª”æ¡ˆ: {current_processing_file}")
        print("   (å†æŒ‰ä¸€æ¬¡ Ctrl+C å¼·åˆ¶é€€å‡º)")
    else:
        print("\nğŸ’¥ å¼·åˆ¶é€€å‡º...")
        import sys
        sys.exit(1)

signal.signal(signal.SIGINT, signal_handler)

def get_memory_usage():
    """ç²å–ç›®å‰è¨˜æ†¶é«”ä½¿ç”¨é‡"""
    try:
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    except Exception:
        return 0
        
def delayed_compare(file_path, delay=10):
    def do_compare():
        compare_excel_changes(file_path, silent=True)
        delayed_checks.pop(file_path, None)  # æ¸…ç†è¨˜éŒ„
    # å¦‚æœèˆŠçš„é‚„åœ¨ï¼Œå–æ¶ˆèˆŠçš„
    if file_path in delayed_checks:
        delayed_checks[file_path].cancel()
    # è¨­å®šæ–°çš„timer
    timer = threading.Timer(delay, do_compare)
    delayed_checks[file_path] = timer
    timer.start()
    
def check_memory_limit():
    """æª¢æŸ¥è¨˜æ†¶é«”æ˜¯å¦è¶…é™"""
    if not ENABLE_MEMORY_MONITOR:
        return False
    current_memory = get_memory_usage()
    if current_memory > MEMORY_LIMIT_MB:
        print(f"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨é‡éé«˜: {current_memory:.1f} MB > {MEMORY_LIMIT_MB} MB")
        print("   æ­£åœ¨åŸ·è¡Œåƒåœ¾å›æ”¶...")
        gc.collect()
        new_memory = get_memory_usage()
        print(f"   åƒåœ¾å›æ”¶å¾Œ: {new_memory:.1f} MB")
        return new_memory > MEMORY_LIMIT_MB
    return False

def save_progress(completed_files, total_files):
    """å„²å­˜é€²åº¦åˆ°æª”æ¡ˆ"""
    if not ENABLE_RESUME:
        return
    try:
        progress_data = {
            "timestamp": datetime.now().isoformat(),
            "completed": completed_files,
            "total": total_files
        }
        with open(RESUME_LOG_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[WARN] ç„¡æ³•å„²å­˜é€²åº¦: {e}")

def load_progress():
    """è¼‰å…¥ä¹‹å‰çš„é€²åº¦"""
    if not ENABLE_RESUME or not os.path.exists(RESUME_LOG_FILE):
        return None
    try:
        with open(RESUME_LOG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"[WARN] ç„¡æ³•è¼‰å…¥é€²åº¦: {e}")
        return None

def timeout_handler():
    """è¶…æ™‚è™•ç†å‡½æ•¸ (åªåœ¨ baseline éšæ®µé‹è¡Œ)"""
    global current_processing_file, processing_start_time, force_stop, baseline_completed
    while not force_stop and not baseline_completed:
        time.sleep(10)
        if current_processing_file and processing_start_time:
            elapsed = time.time() - processing_start_time
            if elapsed > FILE_TIMEOUT_SECONDS:
                print(f"\nâ° æª”æ¡ˆè™•ç†è¶…æ™‚!")
                print(f"   æª”æ¡ˆ: {current_processing_file}")
                print(f"   å·²è™•ç†æ™‚é–“: {elapsed:.1f} ç§’ > {FILE_TIMEOUT_SECONDS} ç§’")
                print(f"   å°‡è·³éæ­¤æª”æ¡ˆä¸¦ç¹¼çºŒ...")
                current_processing_file = None
                processing_start_time = None

def get_all_excel_files(folders):
    all_files = []
    for folder in folders:
        if os.path.isfile(folder):
            if folder.lower().endswith(SUPPORTED_EXTS) and not os.path.basename(folder).startswith('~$'):
                all_files.append(folder)
        elif os.path.isdir(folder):
            for dirpath, _, filenames in os.walk(folder):
                for f in filenames:
                    if f.lower().endswith(SUPPORTED_EXTS) and not f.startswith('~$'):
                        all_files.append(os.path.join(dirpath, f))
    return all_files

def serialize_cell_value(value):
    if value is None:
        return None
    if type(value).__name__ == "ArrayFormula":
        return str(value.formula)
    if hasattr(value, 'formula'):
        return str(value.formula)
    if isinstance(value, datetime):
        return value.isoformat()
    if isinstance(value, (int, float, str, bool)):
        return value
    return str(value)

def get_excel_last_author(path):
    try:
        wb = load_workbook(path, read_only=True)
        author = wb.properties.lastModifiedBy
        wb.close()
        return author
    except Exception:
        return None

def copy_to_cache(network_path, silent=False):
    if not USE_LOCAL_CACHE:
        return network_path
    try:
        os.makedirs(CACHE_FOLDER, exist_ok=True)
        if not os.path.exists(network_path): raise FileNotFoundError(f"ç¶²çµ¡æª”æ¡ˆä¸å­˜åœ¨: {network_path}")
        if not os.access(network_path, os.R_OK): raise PermissionError(f"ç„¡æ³•è®€å–ç¶²çµ¡æª”æ¡ˆ: {network_path}")
        file_hash = hashlib.md5(network_path.encode('utf-8')).hexdigest()[:16]
        cache_file = os.path.join(CACHE_FOLDER, f"{file_hash}_{os.path.basename(network_path)}")
        if os.path.exists(cache_file):
            try:
                if os.path.getmtime(cache_file) >= os.path.getmtime(network_path):
                    return cache_file
            except Exception: pass
        network_size = os.path.getsize(network_path)
        if not silent:
            print(f"   ğŸ“¥ è¤‡è£½åˆ°ç·©å­˜: {os.path.basename(network_path)} ({network_size/(1024*1024):.1f} MB)")
        copy_start = time.time()
        shutil.copy2(network_path, cache_file)
        copy_time = time.time() - copy_start
        if not silent:
            print(f"      è¤‡è£½å®Œæˆï¼Œè€—æ™‚ {copy_time:.1f} ç§’")
        return cache_file
    except Exception as e:
        if not silent:
            print(f"   âŒ ç·©å­˜å¤±æ•—: {e}")
        return network_path

def dump_excel_cells_with_timeout(path, show_sheet_detail=True, silent=False):
    global current_processing_file, processing_start_time
    current_processing_file = path
    processing_start_time = time.time()
    try:
        file_size = os.path.getsize(path)
        if not silent:
            print(f"   ğŸ“Š æª”æ¡ˆå¤§å°: {file_size/(1024*1024):.1f} MB")
        local_path = copy_to_cache(path, silent=silent)
        if ENABLE_FAST_MODE:
            if not silent:
                print(f"   ğŸš€ ä½¿ç”¨å¿«é€Ÿæ¨¡å¼è®€å–...")
            wb = load_workbook(local_path, read_only=True, data_only=False)
            result = {}
            worksheet_count = len(wb.worksheets)
            if not silent:
                print(f"   ğŸ“‹ å·¥ä½œè¡¨æ•¸é‡: {worksheet_count}")
            for idx, ws in enumerate(wb.worksheets, 1):
                cell_count = 0
                ws_data = {}
                if ws.max_row > 1 or ws.max_column > 1:
                    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                        for cell in row:
                            if cell.value is not None:
                                formula = str(cell.value) if cell.data_type == "f" else None
                                if formula and not formula.startswith("="): formula = "=" + formula
                                ws_data[cell.coordinate] = {"formula": formula, "value": serialize_cell_value(cell.value)}
                                cell_count += 1
                if show_sheet_detail and not silent:
                    print(f"      è™•ç†å·¥ä½œè¡¨ {idx}/{worksheet_count}: {ws.title}ï¼ˆ{cell_count} æœ‰è³‡æ–™ cellï¼‰")
                if ws_data: result[ws.title] = ws_data
            wb.close()
            if not silent:
                print(f"   âœ… Excel è®€å–å®Œæˆ")
        # ...æ¨™æº–æ¨¡å¼åŒç†åŠ not silentæ¢ä»¶...
        return result
    except Exception as e:
        if not silent:
            print(f"   âŒ Excel è®€å–å¤±æ•—: {e}")
        return {}
    finally:
        current_processing_file = None
        processing_start_time = None


def hash_excel_content(cells_dict):
    try:
        content_str = json.dumps(cells_dict, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    except Exception:
        return None

def baseline_file_path(base_name):
    return os.path.join(LOG_FOLDER, f"{base_name}.baseline.json.gz")

def load_baseline(baseline_file):
    try:
        if os.path.exists(baseline_file):
            with gzip.open(baseline_file, 'rt', encoding='utf-8') as f:
                return json.load(f)
        return None
    except Exception as e:
        print(f"[ERROR][load_baseline] {baseline_file}: {e}")
        return None

def save_baseline(baseline_file, data):
    try:
        with gzip.open(baseline_file, 'wt', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, separators=(',', ':'))
    except Exception as e:
        print(f"[ERROR][save_baseline] error saving {baseline_file}: {e}")

def is_force_baseline_file(filepath):
    try:
        lowerfile = filepath.lower()
        for pattern in FORCE_BASELINE_ON_FIRST_SEEN:
            if pattern.lower() in lowerfile:
                return True
        return False
    except Exception:
        return False

def human_readable_size(num_bytes):
    if num_bytes is None: return "0 B"
    for unit in ['B','KB','MB','GB','TB']:
        if num_bytes < 1024.0:
            return f"{num_bytes:,.2f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.2f} PB"

def create_baseline_for_files_robust(xlsx_files, skip_force_baseline=True):
    """ğŸ›¡ï¸ å¼·åŒ–ç‰ˆ baseline å»ºç«‹ï¼Œå¸¶è¨ºæ–·å’Œæ¢å¾©åŠŸèƒ½ + hash unchanged smart skip"""
    global force_stop, baseline_completed
    total = len(xlsx_files)
    if total == 0:
        print("[INFO] æ²’æœ‰éœ€è¦ baseline çš„æª”æ¡ˆã€‚")
        baseline_completed = True
        return

    print("\n" + "=" * 90)
    print(" BASELINE å»ºç«‹ç¨‹åº (å¼·åŒ–è¨ºæ–·+HASH SMART SKIP) ".center(90, "="))
    print("=" * 90)
    progress = load_progress()
    start_index = 0
    if progress and ENABLE_RESUME:
        print(f"ğŸ”„ ç™¼ç¾ä¹‹å‰çš„é€²åº¦è¨˜éŒ„: å®Œæˆ {progress.get('completed', 0)}/{progress.get('total', 0)}")
        if input("æ˜¯å¦è¦å¾ä¸Šæ¬¡ä¸­æ–·çš„åœ°æ–¹ç¹¼çºŒ? (y/n): ").strip().lower() == 'y':
            start_index = progress.get('completed', 0)
            print(f"   âœ… å¾ç¬¬ {start_index + 1} å€‹æª”æ¡ˆé–‹å§‹")
    if ENABLE_TIMEOUT:
        timeout_thread = threading.Thread(target=timeout_handler, daemon=True)
        timeout_thread.start()
        print(f"â° å•Ÿç”¨è¶…æ™‚ä¿è­·: {FILE_TIMEOUT_SECONDS} ç§’")
    if ENABLE_MEMORY_MONITOR: print(f"ğŸ’¾ å•Ÿç”¨è¨˜æ†¶é«”ç›£æ§: {MEMORY_LIMIT_MB} MB")
    optimizations = [opt for flag, opt in [(USE_LOCAL_CACHE, "æœ¬åœ°ç·©å­˜"), (ENABLE_FAST_MODE, "å¿«é€Ÿæ¨¡å¼")] if flag]
    print(f"ğŸš€ å•Ÿç”¨å„ªåŒ–: {', '.join(optimizations)}")
    print(f"ğŸ“‚ Baseline å„²å­˜ä½ç½®: {os.path.abspath(LOG_FOLDER)}")
    if USE_LOCAL_CACHE: print(f"ğŸ’¾ æœ¬åœ°ç·©å­˜ä½ç½®: {os.path.abspath(CACHE_FOLDER)}")
    print(f"ğŸ“‹ è¦è™•ç†çš„æª”æ¡ˆ: {total} å€‹ Excel (å¾ç¬¬ {start_index + 1} å€‹é–‹å§‹)")
    print(f"â° é–‹å§‹æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\n" + "-" * 90)
    os.makedirs(LOG_FOLDER, exist_ok=True)
    if USE_LOCAL_CACHE: os.makedirs(CACHE_FOLDER, exist_ok=True)
    baseline_total_size, success_count, skip_count, error_count = 0, 0, 0, 0
    start_time = time.time()
    for i in range(start_index, total):
        if force_stop:
            print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡è™Ÿï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
            save_progress(i, total)
            break
        file_path = xlsx_files[i]
        base_name = os.path.basename(file_path)
        baseline_file = baseline_file_path(base_name)
        if check_memory_limit():
            print(f"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨é‡éé«˜ï¼Œæš«åœ 10 ç§’...")
            time.sleep(10)
            if check_memory_limit():
                print(f"âŒ è¨˜æ†¶é«”ä»ç„¶éé«˜ï¼Œåœæ­¢è™•ç†")
                save_progress(i, total)
                break
        file_start_time = time.time()
        start_time_str = datetime.now().strftime('%H:%M:%S')
        print(f"[å®Œæˆ {i+1:>2}/{total}] [åŸå§‹#{i+1:>2}] è™•ç†ä¸­... (è¨˜æ†¶é«”: {get_memory_usage():.1f}MB)")
        print(f"  æª”æ¡ˆ: {base_name}")
        try:
            if skip_force_baseline and is_force_baseline_file(file_path):
                print(f"  çµæœ: [SKIP] (å±¬æ–¼ FORCE_BASELINE_ON_FIRST_SEEN)")
                skip_count += 1
            else:
                old_baseline = load_baseline(baseline_file)
                old_hash = old_baseline['content_hash'] if old_baseline and 'content_hash' in old_baseline else None
                cell_data = dump_excel_cells_with_timeout(file_path)
                if not cell_data and current_processing_file is None:
                    print(f"  çµæœ: [TIMEOUT] (è™•ç†è¶…æ™‚ï¼Œè·³éæ­¤æª”æ¡ˆ)")
                    error_count += 1
                else:
                    curr_hash = hash_excel_content(cell_data)
                    if old_hash == curr_hash and old_hash is not None:
                        print(f"  çµæœ: [SKIP] (Hash unchanged)")
                        skip_count += 1
                    else:
                        curr_author = get_excel_last_author(file_path)
                        save_baseline(baseline_file, {"last_author": curr_author, "content_hash": curr_hash, "cells": cell_data})
                        size = os.path.getsize(baseline_file)
                        baseline_total_size += size
                        print(f"  çµæœ: [OK]")
                        print(f"  Baseline: {os.path.basename(baseline_file)}")
                        print(f"  æª”æ¡ˆå¤§å°: {human_readable_size(size)} | ç´¯ç©: {human_readable_size(baseline_total_size)}")
                        success_count += 1
            consumed_time = time.time() - file_start_time
            print(f"  æ™‚é–“: å¾ {start_time_str} åˆ° {datetime.now().strftime('%H:%M:%S')} è€—æ™‚ {consumed_time:.2f} ç§’\n")
            save_progress(i + 1, total)
        except Exception as e:
            consumed_time = time.time() - file_start_time
            print(f"  çµæœ: [ERROR]\n  éŒ¯èª¤: {e}")
            print(f"  æ™‚é–“: å¾ {start_time_str} åˆ° {datetime.now().strftime('%H:%M:%S')} è€—æ™‚ {consumed_time:.2f} ç§’\n")
            error_count += 1
            save_progress(i + 1, total)
    baseline_completed = True
    total_time = time.time() - start_time
    print("-" * 90)
    print("ğŸ¯ BASELINE å»ºç«‹å®Œæˆ!")
    print(f"â±ï¸  ç¸½è€—æ™‚: {total_time:.2f} ç§’")
    print(f"âœ… æˆåŠŸ: {success_count} å€‹, â­ï¸  è·³é: {skip_count} å€‹, âŒ å¤±æ•—: {error_count} å€‹")
    print(f"ğŸ“¦ ç´¯ç© baseline æª”æ¡ˆå¤§å°: {human_readable_size(baseline_total_size)}")
    if success_count > 0: print(f"ğŸ“Š å¹³å‡æ¯æª”æ¡ˆè™•ç†æ™‚é–“: {total_time/total:.2f} ç§’")
    if ENABLE_RESUME and os.path.exists(RESUME_LOG_FILE):
        try:
            os.remove(RESUME_LOG_FILE)
            print(f"ğŸ§¹ æ¸…ç†é€²åº¦æª”æ¡ˆ")
        except Exception: pass
    print("\n" + "=" * 90 + "\n")

def compare_excel_changes(file_path, silent=True, event_number=None):
    """æ¯”è¼ƒ Excel æª”æ¡ˆèˆ‡ baseline çš„è®Šæ›´"""
    try:
        import os
        base_name = os.path.basename(file_path)
        baseline_file = baseline_file_path(base_name)
        old_baseline = load_baseline(baseline_file)
        if not old_baseline:
            if not silent:
                print(f"[INFO] æ²’æœ‰ baseline: {base_name}ï¼Œå»ºç«‹æ–° baseline...")
            cell_data = dump_excel_cells_with_timeout(file_path, show_sheet_detail=False, silent=silent)
            curr_author = get_excel_last_author(file_path)
            curr_hash = hash_excel_content(cell_data)
            save_baseline(baseline_file, {"last_author": curr_author, "content_hash": curr_hash, "cells": cell_data})
            return
        curr_cells = dump_excel_cells_with_timeout(file_path, show_sheet_detail=False, silent=silent)
        curr_author = get_excel_last_author(file_path)
        curr_hash = hash_excel_content(curr_cells)
        old_hash = old_baseline.get('content_hash', '')
        if curr_hash == old_hash:
            if not silent:
                print(f"[INFO] æª”æ¡ˆç„¡è®Šæ›´: {base_name}")
            return
        # åªåœ¨summaryå°äº‹ä»¶æ¬¡æ•¸
        if event_number is not None:
            print(f"ğŸŸ¢ [ç¬¬{event_number}æ¬¡äº‹ä»¶]")
        print(f"ğŸš¨ [æª”æ¡ˆæœ‰è®Šæ›´] {base_name}")
        print(f"  ä½œè€…: {old_baseline.get('last_author', '')} â†’ {curr_author}")
        print(f"  Hash: {old_hash[:8]}... â†’ {curr_hash[:8]}...")
        changes = []
        old_cells = old_baseline.get('cells', {})
        all_ws_names = set(old_cells.keys()) | set(curr_cells.keys())
        for ws_name in all_ws_names:
            old_ws_cells = old_cells.get(ws_name, {})
            curr_ws_cells = curr_cells.get(ws_name, {})
            all_coords = set(old_ws_cells.keys()) | set(curr_ws_cells.keys())
            for cell_coord in all_coords:
                old_cell = old_ws_cells.get(cell_coord, {"formula": None, "value": None})
                curr_cell = curr_ws_cells.get(cell_coord, {"formula": None, "value": None})
                if old_cell != curr_cell:
                    changes.append({
                        'worksheet': ws_name,
                        'cell': cell_coord,
                        'old_formula': old_cell['formula'],
                        'old_value': old_cell['value'],
                        'new_formula': curr_cell['formula'],
                        'new_value': curr_cell['value']
                    })
        print_cell_changes_summary(changes)
        log_changes_to_csv(file_path, curr_author, changes)
        save_baseline(baseline_file, {"last_author": curr_author, "content_hash": curr_hash, "cells": curr_cells})
    except Exception as e:
        print(f"[ERROR] æ¯”è¼ƒæª”æ¡ˆå¤±æ•—: {file_path} - {e}")

def print_cell_changes_summary(changes, max_show=10):
    """ğŸ¯ æ–°æ ¼å¼çš„ cell è®Šæ›´é¡¯ç¤º"""
    try:
        print(f"  è®Šæ›´ cell æ•¸é‡ï¼š{len(changes)}")
        maxlen = 50  # ä½ å¯ä»¥èª¿æ•´é–¾å€¼
        for i, change in enumerate(changes[:max_show]):
            ws, cell = change['worksheet'], change['cell']
            old_f, old_v = change['old_formula'] or "", change['old_value'] or ""
            new_f, new_v = change['new_formula'] or "", change['new_value'] or ""
            print(f"    [{ws}] {cell}:")
            # è™•ç†å…¬å¼
            if old_f != new_f:
                if len(str(old_f)) > maxlen or len(str(new_f)) > maxlen:
                    print(f"        [å…¬å¼] '{old_f}'\n              -> '{new_f}'")
                else:
                    print(f"        [å…¬å¼] '{old_f}' -> '{new_f}'")
            # è™•ç†å€¼
            if old_v != new_v:
                if len(str(old_v)) > maxlen or len(str(new_v)) > maxlen:
                    print(f"        [å€¼]   '{old_v}'\n              -> '{new_v}'")
                else:
                    print(f"        [å€¼]   '{old_v}' -> '{new_v}'")
        if len(changes) > max_show:
            print(f"    ... å…¶é¤˜ {len(changes) - max_show} å€‹ cell çœç•¥ ...")
    except Exception as e:
        print(f"[ERROR][print_cell_changes_summary] {e}")

def log_changes_to_csv(file_path, author, changes):
    """è¨˜éŒ„è®Šæ›´åˆ° CSV"""
    try:
        os.makedirs(LOG_FOLDER, exist_ok=True)
        is_new_file = not os.path.exists(CSV_LOG_FILE)
        with gzip.open(CSV_LOG_FILE, 'at', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if is_new_file:
                writer.writerow(['Timestamp', 'File Path', 'Author', 'Worksheet', 'Cell', 'Old Formula', 'Old Value', 'New Formula', 'New Value'])
            for change in changes:
                writer.writerow([datetime.now().strftime('%Y-%m-%d %H:%M:%S'), file_path, author, change['worksheet'], change['cell'], change['old_formula'], change['old_value'], change['new_formula'], change['new_value']])
    except Exception as e:
        print(f"[ERROR] è¨˜éŒ„ CSV å¤±æ•—: {e}")

event_counter = 1  # æ”¾æœ€é ‚åšå…¨åŸŸè®Šæ•¸

class ExcelChangeHandler(FileSystemEventHandler):
    """è™•ç†æª”æ¡ˆç³»çµ±äº‹ä»¶çš„ Handler"""
    def __init__(self):
        self.processing_files = {}
        self.lock = threading.Lock()

    def on_modified(self, event):
        file_path = event.src_path
        global event_counter
        if event.is_directory or not event.src_path.lower().endswith(SUPPORTED_EXTS) or os.path.basename(event.src_path).startswith('~$'):
            return

        file_path = event.src_path
        current_time = time.time()

        with self.lock:
            last_processed_time = self.processing_files.get(file_path, 0)
            if current_time - last_processed_time < 5:  # 5ç§’å…§ä¸é‡è¤‡è™•ç†
                return
            self.processing_files[file_path] = current_time

        print("\n" + "="*40)
        print(f"ğŸŸ¢ [ç¬¬{event_counter}æ¬¡äº‹ä»¶]")
        print(f"ğŸ“ [æª”æ¡ˆä¿®æ”¹äº‹ä»¶] {os.path.basename(file_path)}")
        print(f"   å®Œæ•´è·¯å¾‘: {file_path}")
        print(f"   æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        compare_excel_changes(file_path, silent=True, event_number=event_counter)
        delayed_compare(file_path, delay=10)
        event_counter += 1

def start_watchdog_monitor():
    """å•Ÿå‹• Watchdog ç›£æ§"""
    global force_stop
    force_stop = False
    print("\n" + "=" * 80)
    print(" å•Ÿå‹• Excel æª”æ¡ˆç›£æ§ ".center(80, "="))
    print("=" * 80)
    valid_folders = [folder for folder in WATCH_FOLDERS if os.path.exists(folder)]
    if not valid_folders:
        print("âŒ æ²’æœ‰æœ‰æ•ˆçš„ç›£æ§è³‡æ–™å¤¾ï¼Œç„¡æ³•å•Ÿå‹•ç›£æ§")
        return
    print("  ç›£æ§è³‡æ–™å¤¾:")
    for folder in valid_folders: print(f"    ğŸ“‚ {folder}")
    print(f"\n  æ”¯æ´æª”æ¡ˆ: {SUPPORTED_EXTS}")
    print(f"  è®Šæ›´è¨˜éŒ„: {CSV_LOG_FILE}")
    event_handler = ExcelChangeHandler()
    observer = Observer()
    for folder in valid_folders:
        observer.schedule(event_handler, folder, recursive=True)
    print("\nğŸ” ç›£æ§ä¸­... (æŒ‰ Ctrl+C åœæ­¢)\n" + "-" * 80)
    observer.start()
    try:
        while not force_stop:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ”¶åˆ° Ctrl+C åœæ­¢ä¿¡è™Ÿ...")
    finally:
        observer.stop()
        observer.join()
        print("ğŸ“„ ç›£æ§å·²åœæ­¢")

def print_console_header():
    print("\n" + "="*80)
    print(" Excel File Change Watcher (è¨ºæ–·å¼·åŒ–ç‰ˆæœ¬) ".center(80, "-"))
    print("="*80)
    print(f"  ç›®å‰ä½¿ç”¨è€…: {os.getlogin()}")

if __name__ == "__main__":
    try:
        print_console_header()
        os.makedirs(LOG_FOLDER, exist_ok=True)
        if USE_LOCAL_CACHE: os.makedirs(CACHE_FOLDER, exist_ok=True)
        if SCAN_ALL_MODE:
            all_files = get_all_excel_files(WATCH_FOLDERS)
            print(f"ç¸½å…±æ‰¾åˆ° {len(all_files)} å€‹ Excel æª”æ¡ˆã€‚")
            # baseline mode é¡¯ç¤ºè©³ç´°
            def dump_with_detail(path): return dump_excel_cells_with_timeout(path, show_sheet_detail=True)
            # ä½ å¯ä»¥æŠŠ dump_excel_cells_with_timeout æ›æˆ dump_with_detail
            # æˆ–è€…åœ¨ create_baseline_for_files_robust è£¡èª¿ç”¨æ™‚å‚³ show_sheet_detail=True
            create_baseline_for_files_robust(all_files, skip_force_baseline=True)
        else:
            target_files = get_all_excel_files(MANUAL_BASELINE_TARGET)
            print(f"æ‰‹å‹•æŒ‡å®š baselineï¼Œåˆå…± {len(target_files)} å€‹ Excel æª”æ¡ˆã€‚")
            create_baseline_for_files_robust(target_files, skip_force_baseline=False)

        if force_stop:
            print("ğŸ›‘ ç¨‹åºåœ¨ baseline éšæ®µè¢«ä¸­æ–·ï¼Œé€€å‡º...")
        else:
            start_watchdog_monitor()

    except KeyboardInterrupt:
        print("\nğŸ›‘ ç¨‹åºè¢«ç”¨æˆ¶ä¸­æ–·")
    except Exception as e:
        print(f"\n[CRITICAL ERROR][main] ç¨‹å¼ä¸»æµç¨‹ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nç¨‹åºçµæŸã€‚")
