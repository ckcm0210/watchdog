# -*- coding: utf-8 -*-
"""
Created on Thu Jul 10 16:29:38 2025

@author: kccheng
"""

import os
import time
import csv
import hashlib
import gc
import psutil
import shutil
import gzip
import json
import signal
import threading
from datetime import datetime
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from openpyxl import load_workbook

# =========== User Config ============
SCAN_ALL_MODE = True

# üöÄ ÂÑ™ÂåñÈÅ∏È†Ö
USE_LOCAL_CACHE = True
ENABLE_FAST_MODE = True
CACHE_FOLDER = r"D:\Pzone\Log"

# üîß Ë®∫Êñ∑ÂíåÊÅ¢Âæ©ÈÅ∏È†Ö
ENABLE_TIMEOUT = True          # ÂïüÁî®Ë∂ÖÊôÇ‰øùË≠∑
FILE_TIMEOUT_SECONDS = 120     # ÊØèÂÄãÊ™îÊ°àÊúÄÂ§ßËôïÁêÜÊôÇÈñì (Áßí)
ENABLE_MEMORY_MONITOR = True   # ÂïüÁî®Ë®òÊÜ∂È´îÁõ£Êéß
MEMORY_LIMIT_MB = 2048         # Ë®òÊÜ∂È´îÈôêÂà∂ (MB)
ENABLE_RESUME = True           # ÂïüÁî®Êñ∑ÈªûÁ∫åÂÇ≥
RESUME_LOG_FILE = r"D:\Pzone\Log\baseline_progress.log"  # ÈÄ≤Â∫¶Ë®òÈåÑÊ™î

WATCH_FOLDERS = [
    r"V:\MD9\Constant (T-1 base)\2025Q2\Ballpark\x2025.07.08 (after retention)"
]

MANUAL_BASELINE_TARGET = []

LOG_FOLDER = r"D:\Pzone\Log\excel_watch_log"
LOG_FILE_DATE = datetime.now().strftime('%Y%m%d')
CSV_LOG_FILE = os.path.join(LOG_FOLDER, f"excel_change_log_{LOG_FILE_DATE}.csv.gz")
SUPPORTED_EXTS = ('.xlsx', '.xlsm')

MAX_RETRY = 10
RETRY_INTERVAL_SEC = 2
USE_TEMP_COPY = True

WHITELIST_USERS = ['ckcm0210', 'yourwhiteuser']
LOG_WHITELIST_USER_CHANGE = True

FORCE_BASELINE_ON_FIRST_SEEN = [
    r"\\network_drive\\your_folder1\\must_first_baseline.xlsx",
    "force_this_file.xlsx"
]
# =========== End User Config ============

# ÂÖ®Â±ÄËÆäÊï∏
current_processing_file = None
processing_start_time = None
force_stop = False
baseline_completed = False

def signal_handler(signum, frame):
    """ËôïÁêÜ Ctrl+C ‰∏≠Êñ∑"""
    global force_stop
    if not force_stop:
        force_stop = True
        print("\nüõë Êî∂Âà∞‰∏≠Êñ∑‰ø°ËôüÔºåÊ≠£Âú®ÂÆâÂÖ®ÂÅúÊ≠¢...")
        if current_processing_file:
            print(f"   ÁõÆÂâçËôïÁêÜÊ™îÊ°à: {current_processing_file}")
        print("   (ÂÜçÊåâ‰∏ÄÊ¨° Ctrl+C Âº∑Âà∂ÈÄÄÂá∫)")
    else:
        print("\nüí• Âº∑Âà∂ÈÄÄÂá∫...")
        import sys
        sys.exit(1)

signal.signal(signal.SIGINT, signal_handler)

def get_memory_usage():
    """Áç≤ÂèñÁõÆÂâçË®òÊÜ∂È´î‰ΩøÁî®Èáè"""
    try:
        process = psutil.Process(os.getpid())
        return process.memory_info().rss / 1024 / 1024
    except Exception:
        return 0

def check_memory_limit():
    """Ê™¢Êü•Ë®òÊÜ∂È´îÊòØÂê¶Ë∂ÖÈôê"""
    if not ENABLE_MEMORY_MONITOR:
        return False
    current_memory = get_memory_usage()
    if current_memory > MEMORY_LIMIT_MB:
        print(f"‚ö†Ô∏è Ë®òÊÜ∂È´î‰ΩøÁî®ÈáèÈÅéÈ´ò: {current_memory:.1f} MB > {MEMORY_LIMIT_MB} MB")
        print("   Ê≠£Âú®Âü∑Ë°åÂûÉÂúæÂõûÊî∂...")
        gc.collect()
        new_memory = get_memory_usage()
        print(f"   ÂûÉÂúæÂõûÊî∂Âæå: {new_memory:.1f} MB")
        return new_memory > MEMORY_LIMIT_MB
    return False

def save_progress(completed_files, total_files):
    """ÂÑ≤Â≠òÈÄ≤Â∫¶Âà∞Ê™îÊ°à"""
    if not ENABLE_RESUME:
        return
    try:
        progress_data = {
            "timestamp": datetime.now().isoformat(),
            "completed": completed_files,
            "total": total_files
        }
        with open(RESUME_LOG_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[WARN] ÁÑ°Ê≥ïÂÑ≤Â≠òÈÄ≤Â∫¶: {e}")

def load_progress():
    """ËºâÂÖ•‰πãÂâçÁöÑÈÄ≤Â∫¶"""
    if not ENABLE_RESUME or not os.path.exists(RESUME_LOG_FILE):
        return None
    try:
        with open(RESUME_LOG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"[WARN] ÁÑ°Ê≥ïËºâÂÖ•ÈÄ≤Â∫¶: {e}")
        return None

def timeout_handler():
    """Ë∂ÖÊôÇËôïÁêÜÂáΩÊï∏ (Âè™Âú® baseline ÈöéÊÆµÈÅãË°å)"""
    global current_processing_file, processing_start_time, force_stop, baseline_completed
    while not force_stop and not baseline_completed:
        time.sleep(10)
        if current_processing_file and processing_start_time:
            elapsed = time.time() - processing_start_time
            if elapsed > FILE_TIMEOUT_SECONDS:
                print(f"\n‚è∞ Ê™îÊ°àËôïÁêÜË∂ÖÊôÇ!")
                print(f"   Ê™îÊ°à: {current_processing_file}")
                print(f"   Â∑≤ËôïÁêÜÊôÇÈñì: {elapsed:.1f} Áßí > {FILE_TIMEOUT_SECONDS} Áßí")
                print(f"   Â∞áË∑≥ÈÅéÊ≠§Ê™îÊ°à‰∏¶ÁπºÁ∫å...")
                current_processing_file = None
                processing_start_time = None

def get_all_excel_files(folders):
    all_files = []
    for folder in folders:
        if os.path.isfile(folder):
            if folder.lower().endswith(SUPPORTED_EXTS) and not os.path.basename(folder).startswith('~$'):
                all_files.append(folder)
        elif os.path.isdir(folder):
            for dirpath, _, filenames in os.walk(folder):
                for f in filenames:
                    if f.lower().endswith(SUPPORTED_EXTS) and not f.startswith('~$'):
                        all_files.append(os.path.join(dirpath, f))
    return all_files

def serialize_cell_value(value):
    if value is None: return None
    if isinstance(value, datetime): return value.isoformat()
    if isinstance(value, (int, float, str, bool)): return value
    return str(value)

def get_excel_last_author(path):
    try:
        wb = load_workbook(path, read_only=True)
        author = wb.properties.lastModifiedBy
        wb.close()
        return author
    except Exception:
        return None

def copy_to_cache(network_path):
    if not USE_LOCAL_CACHE:
        return network_path
    try:
        os.makedirs(CACHE_FOLDER, exist_ok=True)
        if not os.path.exists(network_path): raise FileNotFoundError(f"Á∂≤Áµ°Ê™îÊ°à‰∏çÂ≠òÂú®: {network_path}")
        if not os.access(network_path, os.R_OK): raise PermissionError(f"ÁÑ°Ê≥ïËÆÄÂèñÁ∂≤Áµ°Ê™îÊ°à: {network_path}")
        file_hash = hashlib.md5(network_path.encode('utf-8')).hexdigest()[:16]
        cache_file = os.path.join(CACHE_FOLDER, f"{file_hash}_{os.path.basename(network_path)}")
        if os.path.exists(cache_file):
            try:
                if os.path.getmtime(cache_file) >= os.path.getmtime(network_path):
                    return cache_file
            except Exception: pass
        network_size = os.path.getsize(network_path)
        print(f"   üì• Ë§áË£ΩÂà∞Á∑©Â≠ò: {os.path.basename(network_path)} ({network_size/(1024*1024):.1f} MB)")
        copy_start = time.time()
        shutil.copy2(network_path, cache_file)
        copy_time = time.time() - copy_start
        print(f"      Ë§áË£ΩÂÆåÊàêÔºåËÄóÊôÇ {copy_time:.1f} Áßí")
        return cache_file
    except Exception as e:
        print(f"   ‚ùå Á∑©Â≠òÂ§±Êïó: {e}")
        return network_path

def dump_excel_cells_with_timeout(path):
    global current_processing_file, processing_start_time
    current_processing_file = path
    processing_start_time = time.time()
    try:
        file_size = os.path.getsize(path)
        print(f"   üìä Ê™îÊ°àÂ§ßÂ∞è: {file_size/(1024*1024):.1f} MB")
        local_path = copy_to_cache(path)
        if ENABLE_FAST_MODE:
            print(f"   üöÄ ‰ΩøÁî®Âø´ÈÄüÊ®°ÂºèËÆÄÂèñ...")
            wb = load_workbook(local_path, read_only=True, data_only=False)
            result = {}
            worksheet_count = len(wb.worksheets)
            print(f"   üìã Â∑•‰ΩúË°®Êï∏Èáè: {worksheet_count}")
            for idx, ws in enumerate(wb.worksheets, 1):
                print(f"      ËôïÁêÜÂ∑•‰ΩúË°® {idx}/{worksheet_count}: {ws.title}")
                ws_data = {}
                cell_count = 0
                if ws.max_row > 1 or ws.max_column > 1:
                    for row in ws.iter_rows(min_row=1, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                        for cell in row:
                            if cell.value is not None:
                                formula = str(cell.value) if cell.data_type == "f" else None
                                if formula and not formula.startswith("="): formula = "=" + formula
                                ws_data[cell.coordinate] = {"formula": formula, "value": serialize_cell_value(cell.value)}
                                cell_count += 1
                print(f"         ÊâæÂà∞ {cell_count} ÂÄãÊúâË≥áÊñôÁöÑ cell")
                if ws_data: result[ws.title] = ws_data
            wb.close()
            print(f"   ‚úÖ Excel ËÆÄÂèñÂÆåÊàê")
        else:
            # Standard mode (more robust)
            print(f"   üìö ‰ΩøÁî®Ê®ôÊ∫ñÊ®°ÂºèËÆÄÂèñ...")
            wb_formula = load_workbook(local_path, data_only=False)
            wb_value = load_workbook(local_path, data_only=True)
            result = {}
            for ws_formula, ws_value in zip(wb_formula.worksheets, wb_value.worksheets):
                ws_data = {}
                for row_formula, row_value in zip(ws_formula.iter_rows(), ws_value.iter_rows()):
                    for cell_formula, cell_value in zip(row_formula, row_value):
                        try:
                            formula = cell_formula.value if cell_formula.data_type == "f" else None
                            value = serialize_cell_value(cell_value.value)
                            if formula or (value not in [None, ""]):
                                if formula:
                                    formula = str(formula)
                                    if not formula.startswith("="): formula = "=" + formula
                                    if not formula.startswith("'="): formula = "'" + formula
                                ws_data[cell_formula.coordinate] = {"formula": formula, "value": value}
                        except Exception: pass
                if ws_data: result[ws_formula.title] = ws_data
            wb_formula.close()
            wb_value.close()
        return result
    except Exception as e:
        print(f"   ‚ùå Excel ËÆÄÂèñÂ§±Êïó: {e}")
        return {}
    finally:
        current_processing_file = None
        processing_start_time = None

def hash_excel_content(cells_dict):
    try:
        content_str = json.dumps(cells_dict, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(content_str.encode('utf-8')).hexdigest()
    except Exception:
        return None

def baseline_file_path(base_name):
    return os.path.join(LOG_FOLDER, f"{base_name}.baseline.json.gz")

def load_baseline(baseline_file):
    try:
        if os.path.exists(baseline_file):
            with gzip.open(baseline_file, 'rt', encoding='utf-8') as f:
                return json.load(f)
        return None
    except Exception as e:
        print(f"[ERROR][load_baseline] {baseline_file}: {e}")
        return None

def save_baseline(baseline_file, data):
    try:
        with gzip.open(baseline_file, 'wt', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, separators=(',', ':'))
    except Exception as e:
        print(f"[ERROR][save_baseline] error saving {baseline_file}: {e}")

def is_force_baseline_file(filepath):
    try:
        lowerfile = filepath.lower()
        for pattern in FORCE_BASELINE_ON_FIRST_SEEN:
            if pattern.lower() in lowerfile:
                return True
        return False
    except Exception:
        return False

def human_readable_size(num_bytes):
    if num_bytes is None: return "0 B"
    for unit in ['B','KB','MB','GB','TB']:
        if num_bytes < 1024.0:
            return f"{num_bytes:,.2f} {unit}"
        num_bytes /= 1024.0
    return f"{num_bytes:.2f} PB"

def create_baseline_for_files_robust(xlsx_files, skip_force_baseline=True):
    """üõ°Ô∏è Âº∑ÂåñÁâà baseline Âª∫Á´ãÔºåÂ∏∂Ë®∫Êñ∑ÂíåÊÅ¢Âæ©ÂäüËÉΩ + hash unchanged smart skip"""
    global force_stop, baseline_completed
    total = len(xlsx_files)
    if total == 0:
        print("[INFO] Ê≤íÊúâÈúÄË¶Å baseline ÁöÑÊ™îÊ°à„ÄÇ")
        baseline_completed = True
        return

    print("\n" + "=" * 90)
    print(" BASELINE Âª∫Á´ãÁ®ãÂ∫è (Âº∑ÂåñË®∫Êñ∑+HASH SMART SKIP) ".center(90, "="))
    print("=" * 90)
    progress = load_progress()
    start_index = 0
    if progress and ENABLE_RESUME:
        print(f"üîÑ ÁôºÁèæ‰πãÂâçÁöÑÈÄ≤Â∫¶Ë®òÈåÑ: ÂÆåÊàê {progress.get('completed', 0)}/{progress.get('total', 0)}")
        if input("ÊòØÂê¶Ë¶ÅÂæû‰∏äÊ¨°‰∏≠Êñ∑ÁöÑÂú∞ÊñπÁπºÁ∫å? (y/n): ").strip().lower() == 'y':
            start_index = progress.get('completed', 0)
            print(f"   ‚úÖ ÂæûÁ¨¨ {start_index + 1} ÂÄãÊ™îÊ°àÈñãÂßã")
    if ENABLE_TIMEOUT:
        timeout_thread = threading.Thread(target=timeout_handler, daemon=True)
        timeout_thread.start()
        print(f"‚è∞ ÂïüÁî®Ë∂ÖÊôÇ‰øùË≠∑: {FILE_TIMEOUT_SECONDS} Áßí")
    if ENABLE_MEMORY_MONITOR: print(f"üíæ ÂïüÁî®Ë®òÊÜ∂È´îÁõ£Êéß: {MEMORY_LIMIT_MB} MB")
    optimizations = [opt for flag, opt in [(USE_LOCAL_CACHE, "Êú¨Âú∞Á∑©Â≠ò"), (ENABLE_FAST_MODE, "Âø´ÈÄüÊ®°Âºè")] if flag]
    print(f"üöÄ ÂïüÁî®ÂÑ™Âåñ: {', '.join(optimizations)}")
    print(f"üìÇ Baseline ÂÑ≤Â≠ò‰ΩçÁΩÆ: {os.path.abspath(LOG_FOLDER)}")
    if USE_LOCAL_CACHE: print(f"üíæ Êú¨Âú∞Á∑©Â≠ò‰ΩçÁΩÆ: {os.path.abspath(CACHE_FOLDER)}")
    print(f"üìã Ë¶ÅËôïÁêÜÁöÑÊ™îÊ°à: {total} ÂÄã Excel (ÂæûÁ¨¨ {start_index + 1} ÂÄãÈñãÂßã)")
    print(f"‚è∞ ÈñãÂßãÊôÇÈñì: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\n" + "-" * 90)
    os.makedirs(LOG_FOLDER, exist_ok=True)
    if USE_LOCAL_CACHE: os.makedirs(CACHE_FOLDER, exist_ok=True)
    baseline_total_size, success_count, skip_count, error_count = 0, 0, 0, 0
    start_time = time.time()
    for i in range(start_index, total):
        if force_stop:
            print("\nüõë Êî∂Âà∞ÂÅúÊ≠¢‰ø°ËôüÔºåÊ≠£Âú®ÂÆâÂÖ®ÈÄÄÂá∫...")
            save_progress(i, total)
            break
        file_path = xlsx_files[i]
        base_name = os.path.basename(file_path)
        baseline_file = baseline_file_path(base_name)
        if check_memory_limit():
            print(f"‚ö†Ô∏è Ë®òÊÜ∂È´î‰ΩøÁî®ÈáèÈÅéÈ´òÔºåÊö´ÂÅú 10 Áßí...")
            time.sleep(10)
            if check_memory_limit():
                print(f"‚ùå Ë®òÊÜ∂È´î‰ªçÁÑ∂ÈÅéÈ´òÔºåÂÅúÊ≠¢ËôïÁêÜ")
                save_progress(i, total)
                break
        file_start_time = time.time()
        start_time_str = datetime.now().strftime('%H:%M:%S')
        print(f"[ÂÆåÊàê {i+1:>2}/{total}] [ÂéüÂßã#{i+1:>2}] ËôïÁêÜ‰∏≠... (Ë®òÊÜ∂È´î: {get_memory_usage():.1f}MB)")
        print(f"  Ê™îÊ°à: {base_name}")
        try:
            if skip_force_baseline and is_force_baseline_file(file_path):
                print(f"  ÁµêÊûú: [SKIP] (Â±¨Êñº FORCE_BASELINE_ON_FIRST_SEEN)")
                skip_count += 1
            else:
                old_baseline = load_baseline(baseline_file)
                old_hash = old_baseline['content_hash'] if old_baseline and 'content_hash' in old_baseline else None
                cell_data = dump_excel_cells_with_timeout(file_path)
                if not cell_data and current_processing_file is None:
                    print(f"  ÁµêÊûú: [TIMEOUT] (ËôïÁêÜË∂ÖÊôÇÔºåË∑≥ÈÅéÊ≠§Ê™îÊ°à)")
                    error_count += 1
                else:
                    curr_hash = hash_excel_content(cell_data)
                    if old_hash == curr_hash and old_hash is not None:
                        print(f"  ÁµêÊûú: [SKIP] (Hash unchanged)")
                        skip_count += 1
                    else:
                        curr_author = get_excel_last_author(file_path)
                        save_baseline(baseline_file, {"last_author": curr_author, "content_hash": curr_hash, "cells": cell_data})
                        size = os.path.getsize(baseline_file)
                        baseline_total_size += size
                        print(f"  ÁµêÊûú: [OK]")
                        print(f"  Baseline: {os.path.basename(baseline_file)}")
                        print(f"  Ê™îÊ°àÂ§ßÂ∞è: {human_readable_size(size)} | Á¥ØÁ©ç: {human_readable_size(baseline_total_size)}")
                        success_count += 1
            consumed_time = time.time() - file_start_time
            print(f"  ÊôÇÈñì: Âæû {start_time_str} Âà∞ {datetime.now().strftime('%H:%M:%S')} ËÄóÊôÇ {consumed_time:.2f} Áßí\n")
            save_progress(i + 1, total)
        except Exception as e:
            consumed_time = time.time() - file_start_time
            print(f"  ÁµêÊûú: [ERROR]\n  ÈåØË™§: {e}")
            print(f"  ÊôÇÈñì: Âæû {start_time_str} Âà∞ {datetime.now().strftime('%H:%M:%S')} ËÄóÊôÇ {consumed_time:.2f} Áßí\n")
            error_count += 1
            save_progress(i + 1, total)
    baseline_completed = True
    total_time = time.time() - start_time
    print("-" * 90)
    print("üéØ BASELINE Âª∫Á´ãÂÆåÊàê!")
    print(f"‚è±Ô∏è  Á∏ΩËÄóÊôÇ: {total_time:.2f} Áßí")
    print(f"‚úÖ ÊàêÂäü: {success_count} ÂÄã, ‚è≠Ô∏è  Ë∑≥ÈÅé: {skip_count} ÂÄã, ‚ùå Â§±Êïó: {error_count} ÂÄã")
    print(f"üì¶ Á¥ØÁ©ç baseline Ê™îÊ°àÂ§ßÂ∞è: {human_readable_size(baseline_total_size)}")
    if success_count > 0: print(f"üìä Âπ≥ÂùáÊØèÊ™îÊ°àËôïÁêÜÊôÇÈñì: {total_time/total:.2f} Áßí")
    if ENABLE_RESUME and os.path.exists(RESUME_LOG_FILE):
        try:
            os.remove(RESUME_LOG_FILE)
            print(f"üßπ Ê∏ÖÁêÜÈÄ≤Â∫¶Ê™îÊ°à")
        except Exception: pass
    print("\n" + "=" * 90 + "\n")

def compare_excel_changes(file_path):
    """ÊØîËºÉ Excel Ê™îÊ°àËàá baseline ÁöÑËÆäÊõ¥"""
    try:
        base_name = os.path.basename(file_path)
        baseline_file = baseline_file_path(base_name)
        old_baseline = load_baseline(baseline_file)
        if not old_baseline:
            print(f"[INFO] Ê≤íÊúâ baseline: {base_name}ÔºåÂª∫Á´ãÊñ∞ baseline...")
            cell_data = dump_excel_cells_with_timeout(file_path)
            curr_author = get_excel_last_author(file_path)
            curr_hash = hash_excel_content(cell_data)
            save_baseline(baseline_file, {"last_author": curr_author, "content_hash": curr_hash, "cells": cell_data})
            return
        curr_cells = dump_excel_cells_with_timeout(file_path)
        curr_author = get_excel_last_author(file_path)
        curr_hash = hash_excel_content(curr_cells)
        old_hash = old_baseline.get('content_hash', '')
        if curr_hash == old_hash:
            print(f"[INFO] Ê™îÊ°àÁÑ°ËÆäÊõ¥: {base_name}")
            return
        print(f"\nüö® [Ê™îÊ°àÊúâËÆäÊõ¥] {base_name}")
        print(f"  ‰ΩúËÄÖ: {old_baseline.get('last_author', '')} ‚Üí {curr_author}")
        print(f"  Hash: {old_hash[:8]}... ‚Üí {curr_hash[:8]}...")
        changes = []
        old_cells = old_baseline.get('cells', {})
        all_cell_keys = set(old_cells.keys()) | set(curr_cells.keys())
        for ws_name in all_cell_keys:
            old_ws_cells = old_cells.get(ws_name, {})
            curr_ws_cells = curr_cells.get(ws_name, {})
            all_coords = set(old_ws_cells.keys()) | set(curr_ws_cells.keys())
            for cell_coord in all_coords:
                old_cell = old_ws_cells.get(cell_coord, {"formula": None, "value": None})
                curr_cell = curr_ws_cells.get(cell_coord, {"formula": None, "value": None})
                if old_cell != curr_cell:
                    changes.append({'worksheet': ws_name, 'cell': cell_coord, 'old_formula': old_cell['formula'], 'old_value': old_cell['value'], 'new_formula': curr_cell['formula'], 'new_value': curr_cell['value']})
        print_cell_changes_summary(changes)
        log_changes_to_csv(file_path, curr_author, changes)
        save_baseline(baseline_file, {"last_author": curr_author, "content_hash": curr_hash, "cells": curr_cells})
    except Exception as e:
        print(f"[ERROR] ÊØîËºÉÊ™îÊ°àÂ§±Êïó: {file_path} - {e}")

def print_cell_changes_summary(changes, max_show=10):
    """üéØ Êñ∞Ê†ºÂºèÁöÑ cell ËÆäÊõ¥È°ØÁ§∫"""
    try:
        print(f"  ËÆäÊõ¥ cell Êï∏ÈáèÔºö{len(changes)}")
        for i, change in enumerate(changes[:max_show]):
            ws, cell = change['worksheet'], change['cell']
            old_f, old_v = change['old_formula'] or "", change['old_value'] or ""
            new_f, new_v = change['new_formula'] or "", change['new_value'] or ""
            print(f"    [{ws}] {cell}:")
            if old_f != new_f: print(f"        [ÂÖ¨Âºè] '{old_f}' -> '{new_f}'")
            if old_v != new_v: print(f"        [ÂÄº]   '{old_v}' -> '{new_v}'")
        if len(changes) > max_show:
            print(f"    ... ÂÖ∂È§ò {len(changes) - max_show} ÂÄã cell ÁúÅÁï• ...")
    except Exception as e:
        print(f"[ERROR][print_cell_changes_summary] {e}")

def log_changes_to_csv(file_path, author, changes):
    """Ë®òÈåÑËÆäÊõ¥Âà∞ CSV"""
    try:
        os.makedirs(LOG_FOLDER, exist_ok=True)
        is_new_file = not os.path.exists(CSV_LOG_FILE)
        with gzip.open(CSV_LOG_FILE, 'at', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            if is_new_file:
                writer.writerow(['Timestamp', 'File Path', 'Author', 'Worksheet', 'Cell', 'Old Formula', 'Old Value', 'New Formula', 'New Value'])
            for change in changes:
                writer.writerow([datetime.now().strftime('%Y-%m-%d %H:%M:%S'), file_path, author, change['worksheet'], change['cell'], change['old_formula'], change['old_value'], change['new_formula'], change['new_value']])
    except Exception as e:
        print(f"[ERROR] Ë®òÈåÑ CSV Â§±Êïó: {e}")

class ExcelChangeHandler(FileSystemEventHandler):
    """ËôïÁêÜÊ™îÊ°àÁ≥ªÁµ±‰∫ã‰ª∂ÁöÑ Handler"""
    def __init__(self):
        self.processing_files = {}
        self.lock = threading.Lock()

    def on_modified(self, event):
        if event.is_directory or not event.src_path.lower().endswith(SUPPORTED_EXTS) or os.path.basename(event.src_path).startswith('~$'):
            return
        
        file_path = event.src_path
        current_time = time.time()

        with self.lock:
            last_processed_time = self.processing_files.get(file_path, 0)
            if current_time - last_processed_time < 5:  # 5ÁßíÂÖß‰∏çÈáçË§áËôïÁêÜ
                return
            self.processing_files[file_path] = current_time

        print(f"\nüìù [Ê™îÊ°à‰øÆÊîπ‰∫ã‰ª∂] {os.path.basename(file_path)}")
        print(f"   ÂÆåÊï¥Ë∑ØÂæë: {file_path}")
        print(f"   ÊôÇÈñì: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        compare_excel_changes(file_path)

def start_watchdog_monitor():
    """ÂïüÂãï Watchdog Áõ£Êéß"""
    global force_stop
    force_stop = False
    print("\n" + "=" * 80)
    print(" ÂïüÂãï Excel Ê™îÊ°àÁõ£Êéß ".center(80, "="))
    print("=" * 80)
    valid_folders = [folder for folder in WATCH_FOLDERS if os.path.exists(folder)]
    if not valid_folders:
        print("‚ùå Ê≤íÊúâÊúâÊïàÁöÑÁõ£ÊéßË≥áÊñôÂ§æÔºåÁÑ°Ê≥ïÂïüÂãïÁõ£Êéß")
        return
    print("  Áõ£ÊéßË≥áÊñôÂ§æ:")
    for folder in valid_folders: print(f"    üìÇ {folder}")
    print(f"\n  ÊîØÊè¥Ê™îÊ°à: {SUPPORTED_EXTS}")
    print(f"  ËÆäÊõ¥Ë®òÈåÑ: {CSV_LOG_FILE}")
    event_handler = ExcelChangeHandler()
    observer = Observer()
    for folder in valid_folders:
        observer.schedule(event_handler, folder, recursive=True)
    print("\nüîç Áõ£Êéß‰∏≠... (Êåâ Ctrl+C ÂÅúÊ≠¢)\n" + "-" * 80)
    observer.start()
    try:
        while not force_stop:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nüõë Êî∂Âà∞ Ctrl+C ÂÅúÊ≠¢‰ø°Ëôü...")
    finally:
        observer.stop()
        observer.join()
        print("üìÑ Áõ£ÊéßÂ∑≤ÂÅúÊ≠¢")

def print_console_header():
    print("\n" + "="*80)
    print(" Excel File Change Watcher (Ë®∫Êñ∑Âº∑ÂåñÁâàÊú¨) ".center(80, "-"))
    print("="*80)
    print(f"  ÁõÆÂâç‰ΩøÁî®ËÄÖ: {os.getlogin()}")

if __name__ == "__main__":
    try:
        print_console_header()
        os.makedirs(LOG_FOLDER, exist_ok=True)
        if USE_LOCAL_CACHE: os.makedirs(CACHE_FOLDER, exist_ok=True)
        if SCAN_ALL_MODE:
            all_files = get_all_excel_files(WATCH_FOLDERS)
            print(f"Á∏ΩÂÖ±ÊâæÂà∞ {len(all_files)} ÂÄã Excel Ê™îÊ°à„ÄÇ")
            create_baseline_for_files_robust(all_files, skip_force_baseline=True)
        else:
            target_files = get_all_excel_files(MANUAL_BASELINE_TARGET)
            print(f"ÊâãÂãïÊåáÂÆö baselineÔºåÂêàÂÖ± {len(target_files)} ÂÄã Excel Ê™îÊ°à„ÄÇ")
            create_baseline_for_files_robust(target_files, skip_force_baseline=False)
        
        if force_stop:
            print("üõë Á®ãÂ∫èÂú® baseline ÈöéÊÆµË¢´‰∏≠Êñ∑ÔºåÈÄÄÂá∫...")
        else:
            start_watchdog_monitor()
            
    except KeyboardInterrupt:
        print("\nüõë Á®ãÂ∫èË¢´Áî®Êà∂‰∏≠Êñ∑")
    except Exception as e:
        print(f"\n[CRITICAL ERROR][main] Á®ãÂºè‰∏ªÊµÅÁ®ãÁôºÁîüÂö¥ÈáçÈåØË™§: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nÁ®ãÂ∫èÁµêÊùü„ÄÇ")
